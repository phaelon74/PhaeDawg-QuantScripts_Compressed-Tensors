calibration_set:
  _templates:
    - role: system
      prompt: ""
      completion: ""

  # =====================================================
  # SWRP: STORYWRITING + ROLEPLAY BLEND
  # Dual-purpose calibration for models excelling at both
  # creative writing AND character roleplay
  # =====================================================
  
  max_seq_length: 4096
  shuffle: true
  seed: 42

  # =====================================================
  # Category Summary (Total: 512 samples)
  # Balanced blend optimized for SW+RP capabilities
  # =====================================================
  # Narrative Fiction (102 samples - 20%)        [SHARED PRIORITY]
  # Roleplay / Character Cards (102 samples - 20%) [RP PRIORITY]
  # Dialogue & Character Voice (92 samples - 18%) [SHARED PRIORITY]
  # World-building & Scenarios (61 samples - 12%) [SHARED PRIORITY]
  # Genre-specific Prompts (51 samples - 10%)    [SHARED]
  # Adventure / Interactive (26 samples - 5%)    [RP FOCUS]
  # Poetry & Prose Rhythm (15 samples - 3%)      [SW FOCUS]
  # Creative Nonfiction (15 samples - 3%)        [SW FOCUS]
  # General Conversation (15 samples - 3%)       [BASELINE]
  # General Capability Preservation (21 samples - 4%) [CRITICAL]
  # Multilingual (12 samples - 2%)               [BREADTH]
  # =====================================================

  datasets:

    # ================================================
    # Narrative Fiction — 102 samples (20%)
    # Core storytelling for both SW and RP context
    # Plot, character arcs, pacing, story structure
    # ================================================
    
    # Premium Claude-quality writing prompts
    - dataset: Gryphe/Opus-WritingPrompts
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 40

    # Community writing prompts - diverse styles
    - dataset: euclaise/writingprompts
      split: train
      columns: [prompt, story]
      formatter: prompt_answer
      num_samples: 35

    # Classic fiction for literary grounding
    - dataset: sam-paech/gutenberg3-generalfiction-scifi-fantasy-romance-adventure-dpo
      split: train
      columns: [chosen]
      formatter: chat_completion
      num_samples: 27


    # ================================================
    # Roleplay / Character Cards — 102 samples (20%)
    # Character-based interaction and persona play
    # Essential for RP model performance
    # ================================================
    
    # High-quality filtered RP interactions (increased samples)
    - dataset: anthracite-org/stheno-filtered-v1.1
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 50

    # Claude Opus quality RP scenarios (increased samples)
    - dataset: anthracite-org/kalo-opus-instruct-22k-no-refusal
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 42

    # Opus writing prompts for RP context
    - dataset: Gryphe/Opus-WritingPrompts
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 10


    # ================================================
    # Dialogue & Character Voice — 92 samples (18%)
    # Natural speech, character authenticity
    # Critical overlap for both SW and RP
    # ================================================
    
    # Natural conversation patterns
    - dataset: stingning/ultrachat
      split: train
      columns: [data]
      formatter: chat_completion
      num_samples: 40

    # Persona-conditioned dialogue (increased)
    - dataset: awsaf49/persona-chat
      split: train
      columns: [persona, conversation]
      formatter: chat_completion
      num_samples: 32

    # Additional conversational data
    - dataset: HuggingFaceH4/ultrachat_200k
      split: train_sft
      columns: [messages]
      formatter: chat_completion
      num_samples: 20


    # ================================================
    # World-building & Scenarios — 61 samples (12%)
    # Setting creation, atmosphere, scene-setting
    # Essential for both immersive stories and RP
    # ================================================
    
    # Claude writing - rich descriptive prose
    - dataset: anthracite-org/nopm_claude_writing_fixed
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 30

    # Adventure scenarios and settings
    - dataset: PocketDoc/Dans-Prosemaxx-Adventure
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 31


    # ================================================
    # Genre-specific Prompts — 51 samples (10%)
    # Fantasy, Sci-Fi, Horror, Romance, Mystery
    # Versatility across genres for both SW and RP
    # ================================================
    
    # GPT-4o diverse genre coverage
    - dataset: Gryphe/ChatGPT-4o-Writing-Prompts
      split: train
      columns: [prompt, story]
      formatter: prompt_answer
      num_samples: 30

    # Augmented writing prompts - genre variety
    - dataset: fabraz/writingPromptAug
      split: train
      columns: [prompt, story]
      formatter: prompt_answer
      num_samples: 21


    # ================================================
    # Adventure / Interactive Fiction — 26 samples (5%)
    # Interactive storytelling, choice-driven narratives
    # Bridges SW narrative with RP interactivity
    # ================================================
    
    # Interactive adventure scenarios
    - dataset: PocketDoc/Dans-Kinomaxx-VanillaBackrooms
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 16

    # Anime character RP scenarios
    - dataset: zerofata/Roleplay-Anime-Characters
      split: train
      columns: [messages]
      formatter: chat_completion
      num_samples: 10


    # ================================================
    # Poetry & Prose Rhythm — 15 samples (3%)
    # Literary devices, sentence flow, stylistic range
    # Elevates prose quality for SW
    # ================================================
    
    - dataset: merve/poetry
      split: train
      columns: [content]
      formatter: raw_text
      num_samples: 10

    - dataset: Abirate/english_quotes
      split: train
      columns: [quote]
      formatter: raw_text
      num_samples: 5


    # ================================================
    # Creative Nonfiction — 15 samples (3%)
    # Essays, memoirs - grounds creative voice
    # Adds depth to SW prose style
    # ================================================
    
    - dataset: fabiochiu/medium-articles
      split: train
      columns: [content, title]
      formatter: raw_text
      num_samples: 10
      streaming: true

    - dataset: sgoel9/paul_graham_essays
      split: train
      columns: [text, title]
      formatter: raw_text
      num_samples: 5


    # ================================================
    # General Conversation — 15 samples (3%)
    # Baseline conversational ability
    # ================================================
    
    - dataset: HuggingFaceH4/ultrachat_200k
      split: train_sft
      columns: [messages]
      formatter: chat_completion
      num_samples: 10

    - dataset: databricks/databricks-dolly-15k
      split: train
      columns: [instruction, response]
      formatter: prompt_answer
      num_samples: 5


    # ================================================
    # General Capability Preservation — 21 samples (4%)
    # Math, reasoning - prevents capability loss
    # Critical for quantization stability
    # ================================================
    
    - dataset: nvidia/HelpSteer
      split: train
      columns: [prompt, response]
      formatter: prompt_answer
      num_samples: 9

    - dataset: garage-bAInd/Open-Platypus
      split: train
      columns: [instruction, output]
      formatter: prompt_answer
      num_samples: 7

    - dataset: nvidia/OpenMathInstruct-2
      split: train
      columns: [problem, generated_solution]
      formatter: prompt_answer
      num_samples: 5
      streaming: true


    # ================================================
    # Multilingual — 12 samples (2%)
    # Language breadth for international content
    # ================================================
    
    - dataset: HuggingFaceH4/Multilingual-Thinking
      split: train
      columns: [user]
      formatter: raw_text
      num_samples: 12
