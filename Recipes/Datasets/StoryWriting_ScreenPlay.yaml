calibration_set:
  _templates:
    - role: system
      prompt: ""
      completion: ""

  # Screenplay/dialogue-focused calibration
  max_seq_length: 4096
  shuffle: true
  seed: 42

  # =====================================================
  # Category Summary (Total: 512 samples)
  # Heavy emphasis on dialogue and screenplay content
  # =====================================================
  # Screenplay & Movie Scripts (200 samples - 39%)
  # Dialogue & Character Voice (100 samples - 19.5%)
  # Roleplay / Improvisation (77 samples - 15%)
  # Narrative Seeds (60 samples - 11.7%)
  # Instruction & Screenplay Craft (26 samples - 5%)
  # General Chat (15 samples - 3%)
  # General Capability Preservation (26 samples - 5%)
  # Multilingual (8 samples - 1.5%)
  # =====================================================

  datasets:

    # ================================================
    # Screenplay & Movie Scripts — 200 samples (39%)
    # Core screenplay calibration content
    # ================================================
    - dataset: cornell-movie-dialog/cornell_movie_dialog
      split: train
      columns: [line_text, character, movie_title]
      formatter: chat_completion
      num_samples: 100

    - dataset: IsmaelMousa/movies
      split: train
      columns: [Script, Name]
      formatter: raw_text
      num_samples: 60

    - dataset: mocboch/movie_scripts
      split: train
      columns: [script, title]
      formatter: raw_text
      num_samples: 40


    # ================================================
    # Dialogue & Character Voice — 100 samples (19.5%)
    # Multi-turn dialogue with character depth
    # ================================================
    - dataset: ConvLab/dailydialog
      split: train
      columns: [dialogues]
      formatter: chat_completion
      num_samples: 50

    - dataset: awsaf49/persona-chat
      split: train
      columns: [persona, conversation]
      formatter: chat_completion
      num_samples: 50


    # ================================================
    # Roleplay / Improvisation — 77 samples (15%)
    # Character-driven scenes and improvisation
    # ================================================
    - dataset: Gryphe/Sonnet3.5-Charcard-Roleplay
      split: train
      columns: [conversation]
      formatter: sharegpt
      num_samples: 45

    - dataset: anthracite-org/stheno-filtered-v1.1
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 32


    # ================================================
    # Narrative Seeds — 60 samples (11.7%)
    # Story prompts for scene development
    # ================================================
    - dataset: Gryphe/Opus-WritingPrompts
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 35

    - dataset: euclaise/writingprompts
      split: train
      columns: [prompt, story]
      formatter: prompt_answer
      num_samples: 25


    # ================================================
    # Instruction & Screenplay Craft — 26 samples (5%)
    # Writing advice and technique
    # ================================================
    - dataset: databricks/databricks-dolly-15k
      split: train
      columns: [instruction, response]
      formatter: prompt_answer
      num_samples: 26


    # ================================================
    # General Chat — 15 samples (3%)
    # Conversational baseline
    # ================================================
    - dataset: HuggingFaceH4/ultrachat_200k
      split: train_sft
      columns: [messages]
      formatter: chat_completion
      num_samples: 15


    # ================================================
    # General Capability Preservation — 26 samples (5%)
    # Math, reasoning - prevents capability loss
    # ================================================
    - dataset: nvidia/HelpSteer
      split: train
      columns: [prompt, response]
      formatter: prompt_answer
      num_samples: 10

    - dataset: nvidia/OpenMathInstruct-2
      split: train
      columns: [problem, generated_solution]
      formatter: prompt_answer
      num_samples: 8
      streaming: true

    - dataset: garage-bAInd/Open-Platypus
      split: train
      columns: [instruction, output]
      formatter: prompt_answer
      num_samples: 8


    # ================================================
    # Multilingual — 8 samples (1.5%)
    # Brief multilingual dialogue
    # ================================================
    - dataset: HuggingFaceH4/Multilingual-Thinking
      split: train
      columns: [user]
      formatter: raw_text
      num_samples: 8

