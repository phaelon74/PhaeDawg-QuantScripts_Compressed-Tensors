```
calibration_set:
  _templates:
    - role: system
      prompt: ""
      completion: ""
  max_seq_length: 8192
  shuffle: true
  seed: 42
  datasets:
    # Narrative Fiction (25 samples - 25.0%)
    - dataset: SaladTechnologies/fiction-1b
      split: train
      columns: [text]
      formatter: raw_text
      num_samples: 8
    - dataset: SimpleStories/SimpleStories
      split: train
      columns: [story]
      formatter: raw_text
      num_samples: 8
    - dataset: agentlans/literary-synthesis
      split: train
      columns: [input, output]
      formatter: prompt_answer
      num_samples: 9
    # Dialogue & Character Voice (15 samples - 15.0%)
    - dataset: agentlans/multi-character-dialogue
      split: train
      columns: [conversation]
      formatter: chat_completion
      num_samples: 8
    - dataset: mrfakename/voice-acting-inst-en
      split: train
      columns: [instruction, text]
      formatter: prompt_answer
      num_samples: 7
    # Creative Nonfiction (10 samples - 10.0%)
    - dataset: databricks/databricks-dolly-15k
      split: train
      columns: [instruction, response]
      formatter: prompt_answer
      num_samples: 10
    # Genre-specific Prompts (10 samples - 10.0%)
    - dataset: andrewelawrence/writingPrompts-merged
      split: train
      columns: [prompt, story]
      formatter: prompt_answer
      num_samples: 10
    # Instruction & Writing Advice (20 samples - 20.0%)
    - dataset: databricks/databricks-dolly-15k
      split: train
      columns: [instruction, response]
      formatter: prompt_answer
      num_samples: 20
    # Roleplay / Improvisation (10 samples - 10.0%)
    - dataset: agentlans/combined-roleplay
      split: train
      columns: [conversations]
      formatter: sharegpt
      num_samples: 10
    # General Chat / Conversational (5 samples - 5.0%)
    - dataset: HuggingFaceH4/ultrachat_200k
      split: train_sft
      columns: [messages]
      formatter: chat_completion
      num_samples: 5
    # Misc / General Knowledge (5 samples - 5.0%)
    - dataset: MuskumPillerum/General-Knowledge
      split: train
      columns: [Question, Answer]
      formatter: prompt_answer
      num_samples: 5
```

Narrative Fiction

The narrative fiction category uses long-form story corpora. For example, SaladTechnologies/fiction-1b provides over 1 billion words of English narrative fiction (sourced from Project Gutenberg, AO3, etc.). We also include SimpleStories – a 2.08M-sample dataset of short stories generated by various models – and agentlans/literary-synthesis, which contains creative writing prompts and sample responses (tagged “creative-writing” and “writing-prompts”). These datasets together cover fictional storytelling in varied styles.

Dialogue & Character Voice

For dialogues and character-driven writing, we include multi-party conversations. agentlans/multi-character-dialogue provides multi-turn roleplay dialogues among defined characters (tagged with “dialogue” and “roleplay”). We also include mrfakename/voice-acting-inst-en, which contains voice-acting instructions and example character lines in English. These support stylistic and spoken-character prompts in fiction.

Creative Nonfiction

We found no dedicated English “creative nonfiction” corpus on Hugging Face. To approximate this style, we reuse the creative writing instruction tasks (from Dolly-15k) in this category. Dolly’s “Creative Writing” task asks for open-ended written responses, which can cover narrative-expository styles. In practice we draw 10 samples from databricks/databricks-dolly-15k to represent this category.

Genre-specific Prompts

This category uses datasets of story-writing prompts tied to genres. We include andrewelawrence/writingPrompts-merged, a large set of Reddit writing prompts with associated story completions (tagged “writing prompts” and “creative-writing”). This provides examples where the system is given a specific story prompt (often with a genre context) and responds with a fiction narrative.

Instruction & Writing Advice

For writing advice and instruction-style prompts, we use the databricks/databricks-dolly-15k dataset of human-generated instructions and responses. Dolly-15k includes many creative-writing tasks (the “Creative Writing” category) as well as other instruction-following examples. We take 20 samples from Dolly as representative instruction-following prompts related to writing (e.g. “Write a paragraph about…”). This covers direct instructions and tips for creative writing.

Roleplay / Improvisation

This category leverages roleplay scenarios and improvisational dialogue. We use agentlans/combined-roleplay, which aggregates several roleplay-style conversation datasets (tagged as “roleplay”, “dialogue”, “conversational”). These samples consist of back-and-forth chat with system instructions, supporting interactive roleplaying prompts.

General Chat / Conversational

For generic conversational style, we include HuggingFaceH4/ultrachat_200k, an English chat dataset drawn from technical-support and travel domain dialogues. We take 5 samples from its training set (formatted as user/assistant message sequences) to cover non-specific chat. This represents typical conversational exchanges outside of any story context.

Misc / General Knowledge

To cover miscellaneous factual knowledge, we include MuskumPillerum/General-Knowledge, a general trivia Q&A dataset in English. For example, it contains pairs like “What is Artificial Intelligence?” with a descriptive answer. Using 5 samples from this set ensures the calibration covers general fact-based Q&A, balancing out the creative-writing focus.

Sources: Dataset descriptions and previews from Hugging Face (shown above) were used to identify and categorize each dataset. Each category’s datasets were chosen to match the thematic content listed.

